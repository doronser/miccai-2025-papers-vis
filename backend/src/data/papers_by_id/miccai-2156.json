{
  "id": "miccai-2156",
  "title": "FunBench: Benchmarking Fundus Reading Skills of MLLMs",
  "abstract": "Multimodal Large Language Models (MLLMs) have shown significant potential in medical image analysis. However, their capabilities in interpreting fundus images, a critical skill for ophthalmology, remain under-evaluated. Existing benchmarks lack fine-grained task divisions and fail to provide modular analysis of its two key modules, i.e., large language model (LLM) and vision encoder (VE). This paper introduces FunBench, a novel visual question answering (VQA) benchmark designed to comprehensively evaluate MLLMsâ€™ fundus reading skills. FunBench features a hierarchical task organization across four levels (modality perception, anatomy perception, lesion analysis, and disease diagnosis). It also offers three targeted evaluation modes: linear-probe based VE evaluation, knowledge-prompted LLM evaluation, and holistic evaluation. Experiments on nine open-source MLLMs plus GPT-4o reveal significant deficiencies in fundus reading skills, particularly in basic tasks such as laterality recognition. The results highlight the limitations of current MLLMs and emphasize the need for domain-specific training and improved LLMs and VEs.",
  "authors": [
    {
      "name": "Wei, Qijie",
      "affiliation": null,
      "email": null
    },
    {
      "name": "Qian, Kaiheng",
      "affiliation": null,
      "email": null
    },
    {
      "name": "Li, Xirong",
      "affiliation": null,
      "email": null
    }
  ],
  "subject_areas": [
    "Body -> Eye",
    "Modalities -> Other",
    "Modalities -> Photograph / Video",
    "Applications -> Computer Aided Diagnosis",
    "Machine Learning -> Deep Learning",
    "Machine Learning -> Validation"
  ],
  "external_links": [
    {
      "type": "pdf",
      "url": "https://papers.miccai.org/miccai-2025/paper/2156_paper.pdf",
      "description": "Full paper PDF"
    }
  ],
  "publication_date": "2025-10-01",
  "raw_data_source": "{}"
}